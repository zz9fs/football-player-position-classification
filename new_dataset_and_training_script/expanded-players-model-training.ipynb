{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc38137-7c7a-4a76-8146-bcc0d130ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Model=LogisticRegression, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__C': 1.0}\n",
      "Accuracy: 0.7790302379674361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.64      0.62       323\n",
      "           1       0.91      0.93      0.92       993\n",
      "           2       0.77      0.67      0.71       467\n",
      "           3       0.78      0.87      0.82       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.85      0.86      0.85       418\n",
      "           6       0.33      0.33      0.33       323\n",
      "           7       0.28      0.11      0.16       123\n",
      "           8       0.76      0.88      0.82       446\n",
      "           9       0.36      0.30      0.33       305\n",
      "          10       0.29      0.06      0.11       124\n",
      "          11       0.86      0.96      0.90       756\n",
      "\n",
      "    accuracy                           0.78      5589\n",
      "   macro avg       0.65      0.63      0.63      5589\n",
      "weighted avg       0.76      0.78      0.77      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8590087672213276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.71      0.73       323\n",
      "           1       0.93      0.96      0.95       993\n",
      "           2       0.88      0.75      0.81       467\n",
      "           3       0.78      0.94      0.85       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.90      0.90      0.90       418\n",
      "           6       0.66      0.63      0.64       323\n",
      "           7       0.92      0.46      0.62       123\n",
      "           8       0.83      0.89      0.86       446\n",
      "           9       0.70      0.56      0.62       305\n",
      "          10       0.89      0.47      0.61       124\n",
      "          11       0.89      0.98      0.93       756\n",
      "\n",
      "    accuracy                           0.86      5589\n",
      "   macro avg       0.84      0.77      0.79      5589\n",
      "weighted avg       0.86      0.86      0.85      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8667024512435141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76       323\n",
      "           1       0.94      0.96      0.95       993\n",
      "           2       0.88      0.77      0.82       467\n",
      "           3       0.82      0.95      0.88       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.91      0.91      0.91       418\n",
      "           6       0.63      0.60      0.62       323\n",
      "           7       0.76      0.44      0.56       123\n",
      "           8       0.84      0.91      0.87       446\n",
      "           9       0.69      0.61      0.65       305\n",
      "          10       0.66      0.49      0.56       124\n",
      "          11       0.92      0.98      0.95       756\n",
      "\n",
      "    accuracy                           0.87      5589\n",
      "   macro avg       0.82      0.78      0.79      5589\n",
      "weighted avg       0.86      0.87      0.86      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__C': 1.0}\n",
      "Accuracy: 0.7652531758811952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.59       323\n",
      "           1       0.94      0.89      0.91       993\n",
      "           2       0.73      0.73      0.73       467\n",
      "           3       0.80      0.80      0.80       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.84      0.86      0.85       418\n",
      "           6       0.41      0.19      0.26       323\n",
      "           7       0.20      0.41      0.27       123\n",
      "           8       0.75      0.90      0.82       446\n",
      "           9       0.37      0.27      0.31       305\n",
      "          10       0.21      0.31      0.25       124\n",
      "          11       0.92      0.90      0.91       756\n",
      "\n",
      "    accuracy                           0.77      5589\n",
      "   macro avg       0.64      0.66      0.64      5589\n",
      "weighted avg       0.77      0.77      0.76      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8702809089282519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       323\n",
      "           1       0.95      0.94      0.95       993\n",
      "           2       0.85      0.80      0.82       467\n",
      "           3       0.82      0.90      0.86       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.87      0.92      0.89       418\n",
      "           6       0.72      0.64      0.68       323\n",
      "           7       0.62      0.59      0.61       123\n",
      "           8       0.84      0.91      0.87       446\n",
      "           9       0.73      0.64      0.68       305\n",
      "          10       0.70      0.59      0.64       124\n",
      "          11       0.93      0.96      0.95       756\n",
      "\n",
      "    accuracy                           0.87      5589\n",
      "   macro avg       0.82      0.80      0.81      5589\n",
      "weighted avg       0.87      0.87      0.87      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8731436750760422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.79       323\n",
      "           1       0.95      0.95      0.95       993\n",
      "           2       0.88      0.80      0.84       467\n",
      "           3       0.84      0.94      0.89       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.92      0.91      0.91       418\n",
      "           6       0.67      0.59      0.62       323\n",
      "           7       0.64      0.58      0.61       123\n",
      "           8       0.85      0.92      0.88       446\n",
      "           9       0.68      0.63      0.66       305\n",
      "          10       0.60      0.57      0.59       124\n",
      "          11       0.93      0.96      0.95       756\n",
      "\n",
      "    accuracy                           0.87      5589\n",
      "   macro avg       0.81      0.80      0.81      5589\n",
      "weighted avg       0.87      0.87      0.87      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__C': 10}\n",
      "Accuracy: 0.7548756485954553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.51       323\n",
      "           1       0.90      0.92      0.91       993\n",
      "           2       0.73      0.63      0.68       467\n",
      "           3       0.70      0.81      0.75       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.85      0.84      0.85       418\n",
      "           6       0.32      0.31      0.32       323\n",
      "           7       0.19      0.07      0.11       123\n",
      "           8       0.75      0.86      0.80       446\n",
      "           9       0.36      0.30      0.32       305\n",
      "          10       0.19      0.04      0.07       124\n",
      "          11       0.85      0.96      0.90       756\n",
      "\n",
      "    accuracy                           0.75      5589\n",
      "   macro avg       0.61      0.60      0.60      5589\n",
      "weighted avg       0.73      0.75      0.74      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8350331007335838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71       323\n",
      "           1       0.91      0.95      0.93       993\n",
      "           2       0.83      0.69      0.75       467\n",
      "           3       0.74      0.91      0.82       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.87      0.88      0.87       418\n",
      "           6       0.65      0.57      0.61       323\n",
      "           7       0.92      0.37      0.53       123\n",
      "           8       0.82      0.86      0.84       446\n",
      "           9       0.66      0.51      0.58       305\n",
      "          10       0.90      0.44      0.59       124\n",
      "          11       0.85      0.97      0.91       756\n",
      "\n",
      "    accuracy                           0.84      5589\n",
      "   macro avg       0.82      0.74      0.76      5589\n",
      "weighted avg       0.83      0.84      0.83      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8393272499552693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70       323\n",
      "           1       0.93      0.95      0.94       993\n",
      "           2       0.84      0.73      0.78       467\n",
      "           3       0.77      0.91      0.84       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.90      0.91      0.91       418\n",
      "           6       0.58      0.53      0.55       323\n",
      "           7       0.63      0.43      0.51       123\n",
      "           8       0.84      0.87      0.85       446\n",
      "           9       0.62      0.54      0.58       305\n",
      "          10       0.64      0.44      0.52       124\n",
      "          11       0.89      0.96      0.92       756\n",
      "\n",
      "    accuracy                           0.84      5589\n",
      "   macro avg       0.78      0.75      0.76      5589\n",
      "weighted avg       0.83      0.84      0.83      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__C': 10}\n",
      "Accuracy: 0.738593666129898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.58      0.53       323\n",
      "           1       0.94      0.89      0.91       993\n",
      "           2       0.66      0.71      0.68       467\n",
      "           3       0.74      0.69      0.71       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.84      0.85      0.85       418\n",
      "           6       0.41      0.17      0.24       323\n",
      "           7       0.19      0.38      0.25       123\n",
      "           8       0.72      0.87      0.79       446\n",
      "           9       0.33      0.24      0.28       305\n",
      "          10       0.19      0.31      0.24       124\n",
      "          11       0.91      0.89      0.90       756\n",
      "\n",
      "    accuracy                           0.74      5589\n",
      "   macro avg       0.62      0.63      0.62      5589\n",
      "weighted avg       0.75      0.74      0.74      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8454106280193237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       323\n",
      "           1       0.94      0.93      0.93       993\n",
      "           2       0.77      0.79      0.78       467\n",
      "           3       0.82      0.84      0.83       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.86      0.90      0.88       418\n",
      "           6       0.69      0.58      0.63       323\n",
      "           7       0.53      0.57      0.55       123\n",
      "           8       0.81      0.87      0.84       446\n",
      "           9       0.70      0.58      0.63       305\n",
      "          10       0.64      0.61      0.63       124\n",
      "          11       0.92      0.95      0.94       756\n",
      "\n",
      "    accuracy                           0.85      5589\n",
      "   macro avg       0.78      0.78      0.78      5589\n",
      "weighted avg       0.84      0.85      0.84      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.8393272499552693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       323\n",
      "           1       0.94      0.94      0.94       993\n",
      "           2       0.81      0.76      0.78       467\n",
      "           3       0.80      0.87      0.84       685\n",
      "           4       1.00      1.00      1.00       626\n",
      "           5       0.90      0.91      0.90       418\n",
      "           6       0.62      0.54      0.57       323\n",
      "           7       0.48      0.54      0.51       123\n",
      "           8       0.82      0.87      0.84       446\n",
      "           9       0.63      0.56      0.60       305\n",
      "          10       0.54      0.49      0.51       124\n",
      "          11       0.92      0.95      0.94       756\n",
      "\n",
      "    accuracy                           0.84      5589\n",
      "   macro avg       0.77      0.76      0.76      5589\n",
      "weighted avg       0.84      0.84      0.84      5589\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of All Experiments:\n",
      "Model=LogisticRegression, PCA=False, SMOTE=False, Accuracy=0.7790302379674361, Params={'clf__C': 1.0}\n",
      "Model=RandomForest, PCA=False, SMOTE=False, Accuracy=0.8590087672213276, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=False, SMOTE=False, Accuracy=0.8667024512435141, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=False, SMOTE=True, Accuracy=0.7652531758811952, Params={'clf__C': 1.0}\n",
      "Model=RandomForest, PCA=False, SMOTE=True, Accuracy=0.8702809089282519, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=False, SMOTE=True, Accuracy=0.8731436750760422, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=True, SMOTE=False, Accuracy=0.7548756485954553, Params={'clf__C': 10}\n",
      "Model=RandomForest, PCA=True, SMOTE=False, Accuracy=0.8350331007335838, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=True, SMOTE=False, Accuracy=0.8393272499552693, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=True, SMOTE=True, Accuracy=0.738593666129898, Params={'clf__C': 10}\n",
      "Model=RandomForest, PCA=True, SMOTE=True, Accuracy=0.8454106280193237, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=True, SMOTE=True, Accuracy=0.8393272499552693, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "########################################\n",
    "# Load and Prepare Data\n",
    "########################################\n",
    "df = pd.read_csv(r'C:\\Users\\Joshua_zza\\Desktop\\IS 597 MLC\\Final Project\\FC25\\expanded_all_players.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "cols_to_drop = ['Name', 'url', 'Team', 'League', 'Nation', 'Alternative positions', 'play style', 'Weak foot']\n",
    "df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "# Extract target\n",
    "target = df['Position']\n",
    "df.drop(columns=['Position'], inplace=True)\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "numeric_cols = [col for col in df.columns if df[col].dtype != 'object']\n",
    "\n",
    "# If you only want 'Preferred foot' as categorical, adjust\n",
    "if 'Preferred foot' in categorical_cols:\n",
    "    categorical_cols = ['Preferred foot']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "########################################\n",
    "# Model Definitions and Parameter Grids\n",
    "########################################\n",
    "def create_nn_model(neurons=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1])) # will adjust if PCA used\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons//2, activation='relu'))\n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# For NN, dimension might change if PCA is applied, so we will define model input_dim dynamically later.\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=1000), {\n",
    "        'clf__C': [0.1, 1.0, 10]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestClassifier(random_state=42), {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__max_depth': [10, 20, None],\n",
    "        'clf__min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(eval_metric='mlogloss', random_state=42), {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__max_depth': [3, 6],\n",
    "        'clf__learning_rate': [0.1, 0.3]\n",
    "    })\n",
    "    # You can add NN after deciding on PCA dimension\n",
    "}\n",
    "\n",
    "########################################\n",
    "# Experiments Config\n",
    "########################################\n",
    "# We will run experiments with/without PCA and with/without SMOTE\n",
    "# This allows us to compare how each choice affects the results\n",
    "experiments = [\n",
    "    {'use_pca': False, 'use_smote': False},\n",
    "    {'use_pca': False, 'use_smote': True},\n",
    "    {'use_pca': True, 'use_smote': False},\n",
    "    {'use_pca': True, 'use_smote': True},\n",
    "]\n",
    "\n",
    "pca_components = 20  # you can adjust this as needed\n",
    "\n",
    "results = []\n",
    "\n",
    "########################################\n",
    "# Run Experiments\n",
    "########################################\n",
    "for exp in experiments:\n",
    "    use_pca = exp['use_pca']\n",
    "    use_smote = exp['use_smote']\n",
    "    \n",
    "    # Create pipeline steps dynamically\n",
    "    steps = [('preprocessing', preprocessor)]\n",
    "    if use_pca:\n",
    "        steps.append(('pca', PCA(n_components=pca_components)))\n",
    "    if use_smote:\n",
    "        steps.append(('smote', SMOTE(random_state=42)))\n",
    "    \n",
    "    # Adjust input_dim for NN if you use NN:\n",
    "    # We'll do this after fitting the preprocessing (and pca if used) pipeline.\n",
    "    \n",
    "    # For each model:\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Running: Model={model_name}, PCA={use_pca}, SMOTE={use_smote}\")\n",
    "        # Use ImbPipeline if SMOTE is used, else normal Pipeline\n",
    "        pipeline_class = ImbPipeline if use_smote else Pipeline\n",
    "        \n",
    "        clf_pipeline = pipeline_class(steps + [('clf', model)])\n",
    "        \n",
    "        # If using PCA, we need to know the dimension after PCA for NN input_dim\n",
    "        # For now, we handle only LR, RF, XGB. If you add NN, you'd need a preliminary fit \n",
    "        # to determine input dimension and re-compile the model if PCA changes dimension.\n",
    "        \n",
    "        grid = GridSearchCV(clf_pipeline, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        y_pred = grid.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        accuracy = report['accuracy']\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Use_PCA': use_pca,\n",
    "            'Use_SMOTE': use_smote,\n",
    "            'Best_Params': grid.best_params_,\n",
    "            'Accuracy': accuracy,\n",
    "            'Classification_Report': report\n",
    "        })\n",
    "        \n",
    "        print(\"Best Params:\", grid.best_params_)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "########################################\n",
    "# After running all experiments, you have a structured `results` variable\n",
    "# You can now analyze which configuration worked best.\n",
    "########################################\n",
    "\n",
    "# Example: Print summary of all experiments\n",
    "print(\"Summary of All Experiments:\")\n",
    "for res in results:\n",
    "    print(f\"Model={res['Model']}, PCA={res['Use_PCA']}, SMOTE={res['Use_SMOTE']}, Accuracy={res['Accuracy']}, Params={res['Best_Params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e54857-3482-44a4-b822-8c0dd1d36992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
