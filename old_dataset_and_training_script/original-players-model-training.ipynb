{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233ee55b-948c-4820-81ea-e9f64a018e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Model=LogisticRegression, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__C': 10}\n",
      "Accuracy: 0.7886133032694476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.65       205\n",
      "           1       0.92      0.93      0.92       637\n",
      "           2       0.78      0.71      0.74       289\n",
      "           3       0.77      0.90      0.83       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.86      0.85      0.85       267\n",
      "           6       0.39      0.36      0.37       207\n",
      "           7       0.11      0.03      0.04        77\n",
      "           8       0.81      0.89      0.85       281\n",
      "           9       0.33      0.28      0.30       192\n",
      "          10       0.25      0.07      0.12        80\n",
      "          11       0.86      0.95      0.90       485\n",
      "\n",
      "    accuracy                           0.79      3548\n",
      "   macro avg       0.64      0.64      0.63      3548\n",
      "weighted avg       0.76      0.79      0.77      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__max_depth': 20, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7573280721533259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.52      0.53       205\n",
      "           1       0.91      0.95      0.93       637\n",
      "           2       0.73      0.64      0.68       289\n",
      "           3       0.65      0.84      0.73       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.85      0.86      0.86       267\n",
      "           6       0.36      0.35      0.36       207\n",
      "           7       0.25      0.01      0.02        77\n",
      "           8       0.79      0.81      0.80       281\n",
      "           9       0.34      0.24      0.28       192\n",
      "          10       0.19      0.04      0.06        80\n",
      "          11       0.80      0.94      0.86       485\n",
      "\n",
      "    accuracy                           0.76      3548\n",
      "   macro avg       0.62      0.60      0.59      3548\n",
      "weighted avg       0.73      0.76      0.74      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=False, SMOTE=False\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 3, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7736753100338218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58       205\n",
      "           1       0.92      0.93      0.92       637\n",
      "           2       0.76      0.71      0.73       289\n",
      "           3       0.74      0.86      0.80       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.86      0.85      0.86       267\n",
      "           6       0.38      0.36      0.37       207\n",
      "           7       0.12      0.08      0.10        77\n",
      "           8       0.81      0.85      0.83       281\n",
      "           9       0.36      0.32      0.34       192\n",
      "          10       0.22      0.10      0.14        80\n",
      "          11       0.85      0.92      0.88       485\n",
      "\n",
      "    accuracy                           0.77      3548\n",
      "   macro avg       0.63      0.63      0.63      3548\n",
      "weighted avg       0.76      0.77      0.76      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__C': 1.0}\n",
      "Accuracy: 0.7584554678692221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61       205\n",
      "           1       0.94      0.89      0.91       637\n",
      "           2       0.71      0.74      0.72       289\n",
      "           3       0.79      0.81      0.80       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.85      0.87      0.86       267\n",
      "           6       0.42      0.19      0.26       207\n",
      "           7       0.12      0.21      0.15        77\n",
      "           8       0.78      0.88      0.83       281\n",
      "           9       0.35      0.26      0.30       192\n",
      "          10       0.18      0.33      0.23        80\n",
      "          11       0.90      0.86      0.88       485\n",
      "\n",
      "    accuracy                           0.76      3548\n",
      "   macro avg       0.63      0.64      0.63      3548\n",
      "weighted avg       0.77      0.76      0.76      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 5, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7553551296505073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       205\n",
      "           1       0.93      0.92      0.93       637\n",
      "           2       0.71      0.71      0.71       289\n",
      "           3       0.68      0.77      0.73       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.84      0.88      0.86       267\n",
      "           6       0.40      0.31      0.35       207\n",
      "           7       0.16      0.16      0.16        77\n",
      "           8       0.77      0.85      0.81       281\n",
      "           9       0.35      0.28      0.31       192\n",
      "          10       0.12      0.07      0.09        80\n",
      "          11       0.86      0.89      0.87       485\n",
      "\n",
      "    accuracy                           0.76      3548\n",
      "   macro avg       0.61      0.62      0.61      3548\n",
      "weighted avg       0.74      0.76      0.75      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=False, SMOTE=True\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7669109357384442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.60      0.60       205\n",
      "           1       0.93      0.92      0.92       637\n",
      "           2       0.73      0.71      0.72       289\n",
      "           3       0.74      0.84      0.79       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.87      0.84      0.85       267\n",
      "           6       0.38      0.33      0.35       207\n",
      "           7       0.13      0.10      0.12        77\n",
      "           8       0.79      0.85      0.82       281\n",
      "           9       0.35      0.33      0.34       192\n",
      "          10       0.14      0.09      0.11        80\n",
      "          11       0.86      0.91      0.88       485\n",
      "\n",
      "    accuracy                           0.77      3548\n",
      "   macro avg       0.63      0.63      0.62      3548\n",
      "weighted avg       0.75      0.77      0.76      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__C': 10}\n",
      "Accuracy: 0.7519729425028185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54       205\n",
      "           1       0.91      0.92      0.92       637\n",
      "           2       0.70      0.65      0.67       289\n",
      "           3       0.66      0.81      0.73       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.86      0.84      0.85       267\n",
      "           6       0.32      0.33      0.32       207\n",
      "           7       0.13      0.04      0.06        77\n",
      "           8       0.79      0.86      0.82       281\n",
      "           9       0.35      0.25      0.29       192\n",
      "          10       0.27      0.07      0.12        80\n",
      "          11       0.84      0.91      0.87       485\n",
      "\n",
      "    accuracy                           0.75      3548\n",
      "   macro avg       0.61      0.60      0.60      3548\n",
      "weighted avg       0.73      0.75      0.74      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7387260428410372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.50       205\n",
      "           1       0.90      0.96      0.93       637\n",
      "           2       0.68      0.57      0.62       289\n",
      "           3       0.60      0.79      0.68       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.83      0.82      0.82       267\n",
      "           6       0.38      0.34      0.36       207\n",
      "           7       0.50      0.01      0.03        77\n",
      "           8       0.80      0.77      0.78       281\n",
      "           9       0.31      0.25      0.28       192\n",
      "          10       0.22      0.03      0.04        80\n",
      "          11       0.81      0.92      0.86       485\n",
      "\n",
      "    accuracy                           0.74      3548\n",
      "   macro avg       0.62      0.58      0.57      3548\n",
      "weighted avg       0.72      0.74      0.72      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=True, SMOTE=False\n",
      "Best Params: {'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7432356257046223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51       205\n",
      "           1       0.91      0.94      0.92       637\n",
      "           2       0.68      0.61      0.64       289\n",
      "           3       0.66      0.79      0.72       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.83      0.84      0.83       267\n",
      "           6       0.39      0.32      0.35       207\n",
      "           7       0.17      0.06      0.09        77\n",
      "           8       0.77      0.78      0.77       281\n",
      "           9       0.31      0.32      0.31       192\n",
      "          10       0.16      0.06      0.09        80\n",
      "          11       0.86      0.90      0.88       485\n",
      "\n",
      "    accuracy                           0.74      3548\n",
      "   macro avg       0.60      0.60      0.59      3548\n",
      "weighted avg       0.73      0.74      0.73      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=LogisticRegression, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__C': 10}\n",
      "Accuracy: 0.7342164599774521\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.56       205\n",
      "           1       0.94      0.88      0.91       637\n",
      "           2       0.63      0.74      0.68       289\n",
      "           3       0.73      0.71      0.72       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.85      0.86      0.85       267\n",
      "           6       0.29      0.12      0.17       207\n",
      "           7       0.12      0.26      0.17        77\n",
      "           8       0.77      0.86      0.81       281\n",
      "           9       0.33      0.26      0.29       192\n",
      "          10       0.21      0.33      0.26        80\n",
      "          11       0.91      0.85      0.88       485\n",
      "\n",
      "    accuracy                           0.73      3548\n",
      "   macro avg       0.61      0.62      0.61      3548\n",
      "weighted avg       0.74      0.73      0.73      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=RandomForest, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.7263246899661782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.55      0.52       205\n",
      "           1       0.93      0.91      0.92       637\n",
      "           2       0.63      0.68      0.65       289\n",
      "           3       0.65      0.67      0.66       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.81      0.85      0.83       267\n",
      "           6       0.34      0.29      0.31       207\n",
      "           7       0.11      0.12      0.11        77\n",
      "           8       0.75      0.80      0.77       281\n",
      "           9       0.33      0.29      0.31       192\n",
      "          10       0.16      0.12      0.14        80\n",
      "          11       0.88      0.86      0.87       485\n",
      "\n",
      "    accuracy                           0.73      3548\n",
      "   macro avg       0.59      0.59      0.59      3548\n",
      "weighted avg       0.72      0.73      0.72      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Running: Model=XGBoost, PCA=True, SMOTE=True\n",
      "Best Params: {'clf__learning_rate': 0.1, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Accuracy: 0.729988726042841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.52       205\n",
      "           1       0.92      0.92      0.92       637\n",
      "           2       0.66      0.66      0.66       289\n",
      "           3       0.68      0.72      0.70       428\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       0.85      0.84      0.84       267\n",
      "           6       0.34      0.30      0.32       207\n",
      "           7       0.10      0.13      0.11        77\n",
      "           8       0.76      0.79      0.78       281\n",
      "           9       0.32      0.27      0.29       192\n",
      "          10       0.16      0.15      0.15        80\n",
      "          11       0.89      0.85      0.87       485\n",
      "\n",
      "    accuracy                           0.73      3548\n",
      "   macro avg       0.60      0.60      0.60      3548\n",
      "weighted avg       0.73      0.73      0.73      3548\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of All Experiments:\n",
      "Model=LogisticRegression, PCA=False, SMOTE=False, Accuracy=0.7886133032694476, Params={'clf__C': 10}\n",
      "Model=RandomForest, PCA=False, SMOTE=False, Accuracy=0.7573280721533259, Params={'clf__max_depth': 20, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=False, SMOTE=False, Accuracy=0.7736753100338218, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 3, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=False, SMOTE=True, Accuracy=0.7584554678692221, Params={'clf__C': 1.0}\n",
      "Model=RandomForest, PCA=False, SMOTE=True, Accuracy=0.7553551296505073, Params={'clf__max_depth': None, 'clf__min_samples_split': 5, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=False, SMOTE=True, Accuracy=0.7669109357384442, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=True, SMOTE=False, Accuracy=0.7519729425028185, Params={'clf__C': 10}\n",
      "Model=RandomForest, PCA=True, SMOTE=False, Accuracy=0.7387260428410372, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=True, SMOTE=False, Accuracy=0.7432356257046223, Params={'clf__learning_rate': 0.3, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Model=LogisticRegression, PCA=True, SMOTE=True, Accuracy=0.7342164599774521, Params={'clf__C': 10}\n",
      "Model=RandomForest, PCA=True, SMOTE=True, Accuracy=0.7263246899661782, Params={'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 300}\n",
      "Model=XGBoost, PCA=True, SMOTE=True, Accuracy=0.729988726042841, Params={'clf__learning_rate': 0.1, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "########################################\n",
    "# Load and Prepare Data\n",
    "########################################\n",
    "df = pd.read_csv(r'C:\\Users\\Joshua_zza\\Desktop\\IS 597 MLC\\Final Project\\FC25\\all_players.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "cols_to_drop = ['Name', 'url', 'Team', 'League', 'Nation', 'Alternative positions', 'play style', 'Weak foot']\n",
    "df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "# Extract target\n",
    "target = df['Position']\n",
    "df.drop(columns=['Position'], inplace=True)\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "numeric_cols = [col for col in df.columns if df[col].dtype != 'object']\n",
    "\n",
    "# If you only want 'Preferred foot' as categorical, adjust\n",
    "if 'Preferred foot' in categorical_cols:\n",
    "    categorical_cols = ['Preferred foot']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "########################################\n",
    "# Model Definitions and Parameter Grids\n",
    "########################################\n",
    "def create_nn_model(neurons=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1])) # will adjust if PCA used\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons//2, activation='relu'))\n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# For NN, dimension might change if PCA is applied, so we will define model input_dim dynamically later.\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=1000), {\n",
    "        'clf__C': [0.1, 1.0, 10]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestClassifier(random_state=42), {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__max_depth': [10, 20, None],\n",
    "        'clf__min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(eval_metric='mlogloss', random_state=42), {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__max_depth': [3, 6],\n",
    "        'clf__learning_rate': [0.1, 0.3]\n",
    "    })\n",
    "    # You can add NN after deciding on PCA dimension\n",
    "}\n",
    "\n",
    "########################################\n",
    "# Experiments Config\n",
    "########################################\n",
    "# We will run experiments with/without PCA and with/without SMOTE\n",
    "# This allows us to compare how each choice affects the results\n",
    "experiments = [\n",
    "    {'use_pca': False, 'use_smote': False},\n",
    "    {'use_pca': False, 'use_smote': True},\n",
    "    {'use_pca': True, 'use_smote': False},\n",
    "    {'use_pca': True, 'use_smote': True},\n",
    "]\n",
    "\n",
    "pca_components = 20  # you can adjust this as needed\n",
    "\n",
    "results = []\n",
    "\n",
    "########################################\n",
    "# Run Experiments\n",
    "########################################\n",
    "for exp in experiments:\n",
    "    use_pca = exp['use_pca']\n",
    "    use_smote = exp['use_smote']\n",
    "    \n",
    "    # Create pipeline steps dynamically\n",
    "    steps = [('preprocessing', preprocessor)]\n",
    "    if use_pca:\n",
    "        steps.append(('pca', PCA(n_components=pca_components)))\n",
    "    if use_smote:\n",
    "        steps.append(('smote', SMOTE(random_state=42)))\n",
    "    \n",
    "    # Adjust input_dim for NN if you use NN:\n",
    "    # We'll do this after fitting the preprocessing (and pca if used) pipeline.\n",
    "    \n",
    "    # For each model:\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Running: Model={model_name}, PCA={use_pca}, SMOTE={use_smote}\")\n",
    "        # Use ImbPipeline if SMOTE is used, else normal Pipeline\n",
    "        pipeline_class = ImbPipeline if use_smote else Pipeline\n",
    "        \n",
    "        clf_pipeline = pipeline_class(steps + [('clf', model)])\n",
    "        \n",
    "        # If using PCA, we need to know the dimension after PCA for NN input_dim\n",
    "        # For now, we handle only LR, RF, XGB. If you add NN, you'd need a preliminary fit \n",
    "        # to determine input dimension and re-compile the model if PCA changes dimension.\n",
    "        \n",
    "        grid = GridSearchCV(clf_pipeline, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        y_pred = grid.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        accuracy = report['accuracy']\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Use_PCA': use_pca,\n",
    "            'Use_SMOTE': use_smote,\n",
    "            'Best_Params': grid.best_params_,\n",
    "            'Accuracy': accuracy,\n",
    "            'Classification_Report': report\n",
    "        })\n",
    "        \n",
    "        print(\"Best Params:\", grid.best_params_)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "########################################\n",
    "# After running all experiments, you have a structured `results` variable\n",
    "# You can now analyze which configuration worked best.\n",
    "########################################\n",
    "\n",
    "# Example: Print summary of all experiments\n",
    "print(\"Summary of All Experiments:\")\n",
    "for res in results:\n",
    "    print(f\"Model={res['Model']}, PCA={res['Use_PCA']}, SMOTE={res['Use_SMOTE']}, Accuracy={res['Accuracy']}, Params={res['Best_Params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe670e-c3ed-412a-a2ed-24df82d59217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
